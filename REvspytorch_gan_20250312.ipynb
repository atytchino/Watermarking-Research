{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYVN3V52LTwc",
    "outputId": "e89acc76-98af-46c1-c2f6-c585a215daec",
    "ExecuteTime": {
     "end_time": "2025-03-12T05:25:50.200128Z",
     "start_time": "2025-03-12T05:25:50.197821Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Epxp6vHdAdIR",
    "outputId": "bcb92ce7-5ebe-4bab-fca9-09994b9e3a56",
    "ExecuteTime": {
     "end_time": "2025-03-12T05:25:50.209423Z",
     "start_time": "2025-03-12T05:25:50.204797Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "path = \"D:\\\\Dropbox\\\\UMA Augusta\\\\PhD\\\\Research Thesis\\\\brain_tumor_mri_dataset\"\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyrTZ5_2_ZqJ",
    "outputId": "6e97cd4b-f81e-403e-fb2e-950c2de1140f",
    "ExecuteTime": {
     "end_time": "2025-03-12T05:26:03.354916Z",
     "start_time": "2025-03-12T05:25:50.464866Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import hashlib\n",
    "from typing import Tuple, Dict, List\n",
    "import multiprocessing\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "class WatermarkMRIDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, image_size: int = 512, watermark_size: int = 64):\n",
    "        self.filepaths = dataframe['filepath'].values\n",
    "        self.image_hashes = dataframe['image_hash'].values\n",
    "        self.image_size = image_size\n",
    "        self.watermark_size = watermark_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def generate_watermark(self, image_hash: str) -> torch.Tensor:\n",
    "        #  hash to numpy array\n",
    "        hash_bytes = bytes.fromhex(image_hash)\n",
    "        hash_array = np.frombuffer(hash_bytes, dtype=np.uint8)\n",
    "        hash_array = hash_array.astype(np.float32) / 255.0\n",
    "\n",
    "        required_size = 16 * 32 * 32\n",
    "        if hash_array.size < required_size:\n",
    "            # Tile the hash_array until we have enough elements\n",
    "            reps = required_size // hash_array.size + 1\n",
    "            hash_array = np.tile(hash_array, reps)\n",
    "\n",
    "        watermark = hash_array[:required_size].reshape(16, 32, 32)\n",
    "        watermark_tensor = torch.from_numpy(watermark)\n",
    "\n",
    "        # Upsample\n",
    "        watermark_tensor = watermark_tensor.unsqueeze(0)  # Add batch dimension\n",
    "        watermark_tensor = F.interpolate(\n",
    "            watermark_tensor,\n",
    "            size=(self.watermark_size, self.watermark_size),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        watermark_tensor = watermark_tensor.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        return watermark_tensor\n",
    "\n",
    "\n",
    "    def process_image(self, image_path: str) -> torch.Tensor:\n",
    "        \"\"\"Load and process image to tensor.\"\"\"\n",
    "        img = Image.open(image_path)\n",
    "\n",
    "        # Convert to RGB if grayscale\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        # Resize\n",
    "        img = img.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
    "\n",
    "        # Convert to tensor\n",
    "        img_tensor = torch.from_numpy(np.array(img)).float().div(255.0)\n",
    "        img_tensor = img_tensor.permute(2, 0, 1)  # Convert to (C,H,W)\n",
    "\n",
    "        return img_tensor\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        img_tensor = self.process_image(self.filepaths[idx])\n",
    "\n",
    "        watermark_tensor = self.generate_watermark(self.image_hashes[idx])\n",
    "\n",
    "        return img_tensor, watermark_tensor\n",
    "\n",
    "class MRIDatasetPreprocessor:\n",
    "    def __init__(self, base_path: str):\n",
    "        # GPU\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Set up paths\n",
    "        self.base_path = Path(base_path)\n",
    "        self.train_path = self.base_path / 'Training'\n",
    "        self.test_path = self.base_path / 'Testing'\n",
    "\n",
    "        #  directories\n",
    "        self.processed_dir = Path('processed_data')\n",
    "        self.watermarks_dir = Path('watermarks')\n",
    "        self.watermarked_dir = Path('watermarked_images')\n",
    "\n",
    "        for dir_path in [self.processed_dir, self.watermarks_dir, self.watermarked_dir]:\n",
    "            dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.image_stats = {}\n",
    "        self.watermark_stats = {}\n",
    "\n",
    "    def create_dataloaders(self, batch_size: int = 32) -> Tuple[DataLoader, DataLoader]:\n",
    "\n",
    "        train_dataset = WatermarkMRIDataset(self.train_df)\n",
    "        test_dataset = WatermarkMRIDataset(self.test_df)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "\n",
    "        return train_loader, test_loader\n",
    "\n",
    "    def pad_to_multiple_32(self, image: torch.Tensor) -> Tuple[torch.Tensor, Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Pad image to multiple of 32 while preserving dimensions info\n",
    "        \"\"\"\n",
    "        _, h, w = image.shape\n",
    "        pad_h = (32 - h % 32) % 32\n",
    "        pad_w = (32 - w % 32) % 32\n",
    "\n",
    "        padding = (0, pad_w, 0, pad_h)  # left, right, top, bottom\n",
    "        padded_image = F.pad(image, padding, mode='reflect')\n",
    "\n",
    "        return padded_image, (pad_h, pad_w)\n",
    "\n",
    "    def analyze_image_dimensions(self, dataframe: pd.DataFrame) -> Dict:\n",
    "\n",
    "        dimensions = []\n",
    "        sizes_kb = []\n",
    "        aspect_ratios = []\n",
    "\n",
    "        for filepath in tqdm(dataframe['filepath'], desc=\"Analyzing images\"):\n",
    "            with Image.open(filepath) as img:\n",
    "                w, h = img.size\n",
    "                dimensions.append((w, h))\n",
    "                sizes_kb.append(os.path.getsize(filepath) / 1024)\n",
    "                aspect_ratios.append(w / h)\n",
    "\n",
    "        dimensions = np.array(dimensions)\n",
    "        sizes_kb = np.array(sizes_kb)\n",
    "\n",
    "        stats = {\n",
    "            'unique_dimensions': np.unique(dimensions, axis=0),\n",
    "            'min_width': dimensions[:, 0].min(),\n",
    "            'max_width': dimensions[:, 0].max(),\n",
    "            'min_height': dimensions[:, 1].min(),\n",
    "            'max_height': dimensions[:, 1].max(),\n",
    "            'mean_width': dimensions[:, 0].mean(),\n",
    "            'mean_height': dimensions[:, 1].mean(),\n",
    "            'min_size_kb': sizes_kb.min(),\n",
    "            'max_size_kb': sizes_kb.max(),\n",
    "            'mean_size_kb': sizes_kb.mean(),\n",
    "            'aspect_ratios': aspect_ratios,\n",
    "            'min_aspect_ratio': min(aspect_ratios),\n",
    "            'max_aspect_ratio': max(aspect_ratios),\n",
    "            'mean_aspect_ratio': np.mean(aspect_ratios)\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def analyze_for_watermarking(self, dataframe: pd.DataFrame) -> Dict:\n",
    "        watermark_stats = {\n",
    "            'min_dimension': float('inf'),\n",
    "            'max_dimension': 0,\n",
    "            'aspect_ratios': [],\n",
    "            'optimal_watermark_sizes': []\n",
    "        }\n",
    "\n",
    "        for filepath in tqdm(dataframe['filepath'], desc=\"Analyzing for watermarking\"):\n",
    "            with Image.open(filepath) as img:\n",
    "                w, h = img.size\n",
    "                min_dim = min(w, h)\n",
    "                max_dim = max(w, h)\n",
    "                aspect_ratio = w / h\n",
    "\n",
    "                watermark_stats['min_dimension'] = min(watermark_stats['min_dimension'], min_dim)\n",
    "                watermark_stats['max_dimension'] = max(watermark_stats['max_dimension'], max_dim)\n",
    "                watermark_stats['aspect_ratios'].append(aspect_ratio)\n",
    "\n",
    "                # Calculate optimal watermark size for this image\n",
    "                optimal_size = max(32, min_dim // 8)\n",
    "                watermark_stats['optimal_watermark_sizes'].append(optimal_size)\n",
    "\n",
    "        watermark_stats['mean_optimal_size'] = np.mean(watermark_stats['optimal_watermark_sizes'])\n",
    "        watermark_stats['median_optimal_size'] = np.median(watermark_stats['optimal_watermark_sizes'])\n",
    "\n",
    "        return watermark_stats\n",
    "\n",
    "    def create_dataset_dataframes(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "        def process_directory(base_path: Path) -> pd.DataFrame:\n",
    "            filepaths = []\n",
    "            labels = []\n",
    "            dimensions = []\n",
    "            image_hashes = []\n",
    "\n",
    "            for fold in os.listdir(base_path):\n",
    "                fold_path = base_path / fold\n",
    "                if not fold_path.is_dir():\n",
    "                    continue\n",
    "\n",
    "                for file in os.listdir(fold_path):\n",
    "                    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        filepath = str(fold_path / file)\n",
    "                        with Image.open(filepath) as img:\n",
    "                            dimensions.append(img.size)\n",
    "                            # Calculate image hash\n",
    "                            img_array = np.array(img)\n",
    "                            img_hash = hashlib.sha256(img_array.tobytes()).hexdigest()\n",
    "                            image_hashes.append(img_hash)\n",
    "\n",
    "                        filepaths.append(filepath)\n",
    "                        labels.append(fold)\n",
    "\n",
    "            return pd.DataFrame({\n",
    "                'filepath': filepaths,\n",
    "                'label': labels,\n",
    "                'dimensions': dimensions,\n",
    "                'image_hash': image_hashes\n",
    "            })\n",
    "\n",
    "        print(\"Processing training set...\")\n",
    "        self.train_df = process_directory(self.train_path)\n",
    "        print(\"Processing testing set...\")\n",
    "        self.test_df = process_directory(self.test_path)\n",
    "\n",
    "        print(\"\\nAnalyzing training set dimensions...\")\n",
    "        self.image_stats['train'] = self.analyze_image_dimensions(self.train_df)\n",
    "        print(\"\\nAnalyzing testing set dimensions...\")\n",
    "        self.image_stats['test'] = self.analyze_image_dimensions(self.test_df)\n",
    "\n",
    "        print(\"\\nAnalyzing training set for watermarking...\")\n",
    "        self.watermark_stats['train'] = self.analyze_for_watermarking(self.train_df)\n",
    "        print(\"\\nAnalyzing testing set for watermarking...\")\n",
    "        self.watermark_stats['test'] = self.analyze_for_watermarking(self.test_df)\n",
    "\n",
    "        return self.train_df, self.test_df\n",
    "\n",
    "    def save_dataset_info(self):\n",
    "        self.train_df.to_csv(self.processed_dir / 'train_info.csv', index=False)\n",
    "        self.test_df.to_csv(self.processed_dir / 'test_info.csv', index=False)\n",
    "\n",
    "        stats_df = pd.DataFrame({\n",
    "            'train': self.image_stats['train'],\n",
    "            'test': self.image_stats['test']\n",
    "        })\n",
    "        stats_df.to_csv(self.processed_dir / 'image_statistics.csv')\n",
    "\n",
    "        watermark_stats_df = pd.DataFrame({\n",
    "            'train': self.watermark_stats['train'],\n",
    "            'test': self.watermark_stats['test']\n",
    "        })\n",
    "        watermark_stats_df.to_csv(self.processed_dir / 'watermark_statistics.csv')\n",
    "\n",
    "    def get_dataset_statistics(self):\n",
    "\n",
    "        print(\"\\n=== Dataset Statistics Summary ===\")\n",
    "        print(f\"Total training images: {len(self.train_df)}\")\n",
    "        print(f\"Total testing images: {len(self.test_df)}\")\n",
    "\n",
    "        for dataset_type in ['train', 'test']:\n",
    "            stats = self.image_stats[dataset_type]\n",
    "            wm_stats = self.watermark_stats[dataset_type]\n",
    "\n",
    "            print(f\"\\n{dataset_type.capitalize()} Set Summary:\")\n",
    "            print(f\"Dimensions:\")\n",
    "            print(f\"  Min Width: {stats['min_width']}, Max Width: {stats['max_width']}\")\n",
    "            print(f\"  Min Height: {stats['min_height']}, Max Height: {stats['max_height']}\")\n",
    "            print(f\"  Mean Width: {stats['mean_width']:.2f}, Mean Height: {stats['mean_height']:.2f}\")\n",
    "\n",
    "            print(f\"\\nFile Sizes:\")\n",
    "            print(f\"  Min: {stats['min_size_kb']:.2f} KB\")\n",
    "            print(f\"  Max: {stats['max_size_kb']:.2f} KB\")\n",
    "            print(f\"  Mean: {stats['mean_size_kb']:.2f} KB\")\n",
    "\n",
    "            print(f\"\\nWatermarking Info:\")\n",
    "            print(f\"  Min Dimension: {wm_stats['min_dimension']}\")\n",
    "            print(f\"  Max Dimension: {wm_stats['max_dimension']}\")\n",
    "            print(f\"  Mean Optimal Watermark Size: {wm_stats['mean_optimal_size']:.2f}\")\n",
    "            print(f\"  Median Optimal Watermark Size: {wm_stats['median_optimal_size']:.2f}\")\n",
    "\n",
    "def test_preprocessor(dataset_path: str):\n",
    "    try:\n",
    "        print(\"Initializing preprocessor...\")\n",
    "        preprocessor = MRIDatasetPreprocessor(dataset_path)\n",
    "\n",
    "        print(\"\\nCreating and analyzing datasets...\")\n",
    "        train_df, test_df = preprocessor.create_dataset_dataframes()\n",
    "\n",
    "        print(\"\\nCreating dataloaders...\")\n",
    "        train_loader, test_loader = preprocessor.create_dataloaders(batch_size=4)\n",
    "\n",
    "        print(\"\\nSaving dataset information...\")\n",
    "        preprocessor.save_dataset_info()\n",
    "\n",
    "        print(\"\\nDisplaying dataset statistics...\")\n",
    "        preprocessor.get_dataset_statistics()\n",
    "\n",
    "        # Test a batch\n",
    "        # print(\"\\nTesting batch loading...\")\n",
    "        # for images, watermarks in train_loader:\n",
    "        #     images = images.to(preprocessor.device)\n",
    "        #     watermarks = watermarks.to(preprocessor.device)\n",
    "        #     print(f\"Image batch shape: {images.shape}\")\n",
    "        #     print(f\"Watermark batch shape: {watermarks.shape}\")\n",
    "        #     print(f\"Image value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "        #     print(f\"Watermark value range: [{watermarks.min():.3f}, {watermarks.max():.3f}]\")\n",
    "        #     break  # Only test first batch\n",
    "\n",
    "        return preprocessor, train_loader, test_loader\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = path\n",
    "    preprocessor, train_loader, test_loader = test_preprocessor(dataset_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing preprocessor...\n",
      "Using device: cuda\n",
      "\n",
      "Creating and analyzing datasets...\n",
      "Processing training set...\n",
      "Processing testing set...\n",
      "\n",
      "Analyzing training set dimensions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing images: 100%|██████████| 5712/5712 [00:01<00:00, 3293.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing testing set dimensions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing images: 100%|██████████| 1311/1311 [00:00<00:00, 2875.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing training set for watermarking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing for watermarking: 100%|██████████| 5712/5712 [00:01<00:00, 4848.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing testing set for watermarking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing for watermarking: 100%|██████████| 1311/1311 [00:00<00:00, 4866.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating dataloaders...\n",
      "\n",
      "Saving dataset information...\n",
      "\n",
      "Displaying dataset statistics...\n",
      "\n",
      "=== Dataset Statistics Summary ===\n",
      "Total training images: 5712\n",
      "Total testing images: 1311\n",
      "\n",
      "Train Set Summary:\n",
      "Dimensions:\n",
      "  Min Width: 150, Max Width: 1920\n",
      "  Min Height: 168, Max Height: 1446\n",
      "  Mean Width: 451.56, Mean Height: 453.88\n",
      "\n",
      "File Sizes:\n",
      "  Min: 3.39 KB\n",
      "  Max: 710.85 KB\n",
      "  Mean: 22.64 KB\n",
      "\n",
      "Watermarking Info:\n",
      "  Min Dimension: 150\n",
      "  Max Dimension: 1920\n",
      "  Mean Optimal Watermark Size: 56.96\n",
      "  Median Optimal Watermark Size: 64.00\n",
      "\n",
      "Test Set Summary:\n",
      "Dimensions:\n",
      "  Min Width: 150, Max Width: 1149\n",
      "  Min Height: 168, Max Height: 1019\n",
      "  Mean Width: 421.18, Mean Height: 424.22\n",
      "\n",
      "File Sizes:\n",
      "  Min: 4.58 KB\n",
      "  Max: 118.71 KB\n",
      "  Mean: 19.48 KB\n",
      "\n",
      "Watermarking Info:\n",
      "  Min Dimension: 150\n",
      "  Max Dimension: 1149\n",
      "  Mean Optimal Watermark Size: 53.42\n",
      "  Median Optimal Watermark Size: 64.00\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVOVozNI_cwc",
    "outputId": "7b09d461-5453-4a98-e82e-cae4d447e3d9",
    "ExecuteTime": {
     "end_time": "2025-03-12T05:26:03.777987Z",
     "start_time": "2025-03-12T05:26:03.366232Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int = 16, qkv_bias: bool = False, attn_drop: float = 0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class MaxViTBlock(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 4., qkv_bias: bool = False,\n",
    "                 drop: float = 0., attn_drop: float = 0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class MaxViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 512,\n",
    "                 embed_dim: int = 768,\n",
    "                 depth: int = 4,\n",
    "                 num_heads: int = 8,\n",
    "                 mlp_ratio: float = 4.,\n",
    "                 qkv_bias: bool = False,\n",
    "                 drop_rate: float = 0.,\n",
    "                 attn_drop_rate: float = 0.):\n",
    "        \"\"\"\n",
    "        Initialize MaxViT model.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial convolution to change channel dimensions\n",
    "        self.conv_in = nn.Conv2d(in_channels, embed_dim, kernel_size=1)\n",
    "\n",
    "        # Position embedding\n",
    "        self.feature_size = 32 * 32  # Fixed size for bottleneck\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.feature_size, embed_dim))\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MaxViTBlock(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Final projection back to input channels\n",
    "        self.conv_out = nn.Conv2d(embed_dim, in_channels, kernel_size=1)\n",
    "\n",
    "        # Initialize position embedding\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        assert H == W == int(self.feature_size ** 0.5), \\\n",
    "            f\"Input spatial dimensions must be {int(self.feature_size ** 0.5)}x{int(self.feature_size ** 0.5)}, got {H}x{W}\"\n",
    "\n",
    "        # Initial convolution: (B, C, H, W) -> (B, embed_dim, H, W)\n",
    "        x = self.conv_in(x)\n",
    "\n",
    "        # Reshape for transformer: (B, embed_dim, H, W) -> (B, H*W, embed_dim)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # Position embedding\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Reshape back: (B, H*W, embed_dim) -> (B, embed_dim, H, W)\n",
    "        x = x.transpose(1, 2).reshape(B, -1, H, W)\n",
    "\n",
    "        # Final projection: (B, embed_dim, H, W) -> (B, C, H, W)\n",
    "        x = self.conv_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class WatermarkProcessor(nn.Module):\n",
    "    def __init__(self, in_channels: int = 16):\n",
    "        super().__init__()\n",
    "        self.process = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 256, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.process(x)\n",
    "\n",
    "def test_maxvit():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    x = torch.randn(2, 512, 32, 32).to(device)\n",
    "    print(f\"Created input tensor with shape: {x.shape}\")\n",
    "\n",
    "    model = MaxViT(in_channels=512, embed_dim=768, depth=4).to(device)\n",
    "    print(\"Initialized MaxViT model\")\n",
    "\n",
    "    print(\"\\nPerforming forward pass...\")\n",
    "    output = model(x)\n",
    "\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Running on: {device}\")\n",
    "\n",
    "    if x.shape == output.shape:\n",
    "        print(\"✓ Input and output shapes match\")\n",
    "    else:\n",
    "        print(\"✗ Input and output shapes differ!\")\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_maxvit()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Created input tensor with shape: torch.Size([2, 512, 32, 32])\n",
      "Initialized MaxViT model\n",
      "\n",
      "Performing forward pass...\n",
      "\n",
      "Summary:\n",
      "Input shape: torch.Size([2, 512, 32, 32])\n",
      "Output shape: torch.Size([2, 512, 32, 32])\n",
      "Running on: cuda\n",
      "✓ Input and output shapes match\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XlsHOeITDG6X",
    "outputId": "5a6419da-0303-4047-957e-6163ebe3df30",
    "ExecuteTime": {
     "end_time": "2025-03-12T05:26:04.143305Z",
     "start_time": "2025-03-12T05:26:03.789300Z"
    }
   },
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int = 8, qkv_bias: bool = False, attn_drop: float = 0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class MaxViTBlock(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 4., qkv_bias: bool = False,\n",
    "                 drop: float = 0., attn_drop: float = 0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class MaxViT(nn.Module):\n",
    "    def __init__(self, in_channels: int = 512, embed_dim: int = 768, depth: int = 4,\n",
    "                 num_heads: int = 8, mlp_ratio: float = 4., qkv_bias: bool = False,\n",
    "                 drop_rate: float = 0., attn_drop_rate: float = 0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, embed_dim, kernel_size=1)\n",
    "\n",
    "        self.feature_size = 32 * 32\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.feature_size, embed_dim))\n",
    "\n",
    "        # MaxViT blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MaxViTBlock(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.conv_out = nn.Conv2d(embed_dim, in_channels, kernel_size=1)\n",
    "\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        assert H * W == self.feature_size, f\"Input feature map size {H}x{W} doesn't match expected size 32x32\"\n",
    "\n",
    "        # Initial convolution\n",
    "        x = self.conv_in(x)  # Shape: (B, embed_dim, H, W)\n",
    "\n",
    "        x = x.flatten(2).transpose(1, 2)  # Shape: (B, H*W, embed_dim)\n",
    "\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, -1, H, W)\n",
    "\n",
    "        x = self.conv_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.gelu(self.bn1(self.conv1(x)))\n",
    "        x = self.gelu(self.bn2(self.conv2(x)))\n",
    "        skip = x\n",
    "        x = self.pool(x)\n",
    "        return x, skip\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, skip_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        # Upsampling\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels // 2 + skip_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.up(x)\n",
    "        # If spatial dimensions differ, pad appropriately.\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            diff_h = skip.size(2) - x.size(2)\n",
    "            diff_w = skip.size(3) - x.size(3)\n",
    "            x = F.pad(x, [diff_w // 2, diff_w - diff_w // 2,\n",
    "                          diff_h // 2, diff_h - diff_h // 2])\n",
    "        # Concatenate along the channel dimension.\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        x = self.gelu(self.bn1(self.conv1(x)))\n",
    "        x = self.gelu(self.bn2(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class AdaptiveWeightBlender(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.weight_conv = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, image: torch.Tensor, watermark: torch.Tensor) -> torch.Tensor:\n",
    "        assert image.shape == watermark.shape, f\"Shape mismatch: image {image.shape}, watermark {watermark.shape}\"\n",
    "\n",
    "        combined = torch.cat([image, watermark], dim=1)\n",
    "\n",
    "        weights = self.weight_conv(combined)\n",
    "\n",
    "        blended = image * weights + watermark * (1 - weights)\n",
    "        return blended\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, maxvit_model: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder pathway\n",
    "        self.enc1 = EncoderBlock(3, 64)      # 512x512 -> 256x256\n",
    "        self.enc2 = EncoderBlock(64, 128)    # 256x256 -> 128x128\n",
    "        self.enc3 = EncoderBlock(128, 256)   # 128x128 -> 64x64\n",
    "        self.enc4 = EncoderBlock(256, 512)   # 64x64 -> 32x32\n",
    "\n",
    "        # Watermark processor (16 -> 512 channels)\n",
    "        self.watermark_processor = nn.Sequential(\n",
    "            nn.Conv2d(16, 256, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.blender = AdaptiveWeightBlender(512)\n",
    "\n",
    "        self.pre_maxvit_conv = nn.Conv2d(512, 768, kernel_size=1)\n",
    "\n",
    "        self.maxvit = maxvit_model\n",
    "\n",
    "        self.post_maxvit_conv = nn.Conv2d(768, 512, kernel_size=1)\n",
    "\n",
    "        # Decoder pathway\n",
    "        self.dec1 = DecoderBlock(in_channels=512, skip_channels=512, out_channels=256)  # 32x32 -> 64x64\n",
    "        self.dec2 = DecoderBlock(in_channels=256, skip_channels=256, out_channels=128)  # 64x64 -> 128x128\n",
    "        self.dec3 = DecoderBlock(in_channels=128, skip_channels=128, out_channels=64)   # 128x128 -> 256x256\n",
    "\n",
    "\n",
    "        # Final output\n",
    "        self.final_conv = nn.Conv2d(64, 3, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        skip_connections = []\n",
    "\n",
    "        x, skip1 = self.enc1(x)\n",
    "        skip_connections.append(skip1)\n",
    "\n",
    "        x, skip2 = self.enc2(x)\n",
    "        skip_connections.append(skip2)\n",
    "\n",
    "        x, skip3 = self.enc3(x)\n",
    "        skip_connections.append(skip3)\n",
    "\n",
    "        x, skip4 = self.enc4(x)\n",
    "        skip_connections.append(skip4)\n",
    "\n",
    "        return x, skip_connections\n",
    "\n",
    "    def bottleneck(self, image_features: torch.Tensor, watermark: torch.Tensor) -> torch.Tensor:\n",
    "        # Process watermark (16 -> 512 channels)\n",
    "        watermark_features = self.watermark_processor(watermark)\n",
    "\n",
    "        # Resize watermark features if needed\n",
    "        if watermark_features.shape[2:] != image_features.shape[2:]:\n",
    "            watermark_features = F.interpolate(\n",
    "                watermark_features,\n",
    "                size=image_features.shape[2:],\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "        blended = self.blender(image_features, watermark_features)\n",
    "\n",
    "        maxvit_input = self.pre_maxvit_conv(blended)\n",
    "\n",
    "        maxvit_output = self.maxvit(maxvit_input)\n",
    "\n",
    "        refined = self.post_maxvit_conv(maxvit_output)\n",
    "\n",
    "        return refined\n",
    "\n",
    "    def decode(self, x: torch.Tensor, skip_connections: List[torch.Tensor]) -> torch.Tensor:\n",
    "        x = self.dec1(x, skip_connections[3])\n",
    "        x = self.dec2(x, skip_connections[2])\n",
    "        x = self.dec3(x, skip_connections[1])\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, image: torch.Tensor, watermark: torch.Tensor) -> torch.Tensor:\n",
    "        encoded, skip_connections = self.encode(image)\n",
    "        bottleneck_features = self.bottleneck(encoded, watermark)\n",
    "        decoded = self.decode(bottleneck_features, skip_connections)  # currently 256×256 output\n",
    "        # Upsample decoded output from 256x256 to 512x512\n",
    "        output = torch.nn.functional.interpolate(decoded, size=(512, 512), mode='bilinear', align_corners=False)\n",
    "        return output\n",
    "\n",
    "\n",
    "def test_generator():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    maxvit = MaxViT(\n",
    "        in_channels=768,\n",
    "        embed_dim=768,\n",
    "        depth=4\n",
    "    ).to(device)\n",
    "\n",
    "    generator = UNetGenerator(maxvit).to(device)\n",
    "\n",
    "    batch_size = 4\n",
    "    image = torch.randn(batch_size, 3, 512, 512).to(device)\n",
    "    watermark = torch.randn(batch_size, 16, 64, 64).to(device)\n",
    "\n",
    "    print(\"\\nStarting forward pass...\")\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = generator(image, watermark)\n",
    "\n",
    "    print(f\"\\nFinal shapes:\")\n",
    "    print(f\"Image input shape: {image.shape}\")\n",
    "    print(f\"Watermark input shape: {watermark.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "    return generator\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_generator()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Starting forward pass...\n",
      "\n",
      "Final shapes:\n",
      "Image input shape: torch.Size([4, 3, 512, 512])\n",
      "Watermark input shape: torch.Size([4, 16, 64, 64])\n",
      "Output shape: torch.Size([4, 3, 512, 512])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UaTTPFT4DTtv",
    "outputId": "72fe2810-ad75-40a9-b9a9-1de82ea4977b",
    "ExecuteTime": {
     "end_time": "2025-03-12T05:26:04.198471Z",
     "start_time": "2025-03-12T05:26:04.152913Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # LeakyReLU\n",
    "        self.leaky_slope = 0.2\n",
    "\n",
    "        # First conv layer: 512x512x3 → 256x256x64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(self.leaky_slope, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Second layer: 256x256x64 → 128x128x128\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(self.leaky_slope, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Third layer: 128x128x128 → 64x64x256\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(self.leaky_slope, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Final layer: 64x64x256 → 31x31x1\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid()  # For binary classification (real/fake)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        x = self.conv1(x)  # → 256x256x64\n",
    "        x = self.conv2(x)  # → 128x128x128\n",
    "        x = self.conv3(x)  # → 64x64x256\n",
    "        x = self.conv4(x)  # → 31x31x1\n",
    "        return x\n",
    "\n",
    "def test_discriminator():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    discriminator = Discriminator().to(device)\n",
    "\n",
    "    batch_size = 4\n",
    "    test_input = torch.randn(batch_size, 3, 512, 512).to(device)\n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = discriminator(test_input)\n",
    "\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output value range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "\n",
    "    assert output.min() >= 0 and output.max() <= 1, \"Output values should be in [0,1] range\"\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_discriminator()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Input shape: torch.Size([4, 3, 512, 512])\n",
      "Output shape: torch.Size([4, 1, 31, 31])\n",
      "Output value range: [0.161, 0.760]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4jmqfVhNCx9d",
    "ExecuteTime": {
     "end_time": "2025-03-12T05:26:04.211338Z",
     "start_time": "2025-03-12T05:26:04.209269Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9UpAMAjgG8Rb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "92b0c92c-efac-40ad-b929-a6622c14ba61",
    "ExecuteTime": {
     "end_time": "2025-03-12T05:28:48.329643Z",
     "start_time": "2025-03-12T05:26:04.222709Z"
    }
   },
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "class WatermarkTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator: nn.Module,\n",
    "        discriminator: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        test_loader: DataLoader,\n",
    "        device: torch.device,\n",
    "        learning_rate: float = 0.0000001,\n",
    "        beta1: float = 0.5,\n",
    "        beta2: float = 0.999\n",
    "    ):\n",
    "        self.generator = generator.to(device)\n",
    "        self.discriminator = discriminator.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "\n",
    "        self.g_optimizer = optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=(beta1, beta2)\n",
    "        )\n",
    "        self.d_optimizer = optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=(beta1, beta2)\n",
    "        )\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "        self.current_epoch = 0\n",
    "        self.train_history = {\n",
    "            'g_loss': [],\n",
    "            'd_loss': [],\n",
    "            'mse_loss': [],\n",
    "            'adv_loss': []\n",
    "        }\n",
    "\n",
    "    def train_discriminator(\n",
    "        self,\n",
    "        real_images: torch.Tensor,\n",
    "        watermark: torch.Tensor\n",
    "    ) -> (torch.Tensor, dict):\n",
    "        batch_size = real_images.size(0)\n",
    "        real_label = torch.ones(batch_size, 1, 31, 31).to(self.device)\n",
    "        fake_label = torch.zeros(batch_size, 1, 31, 31).to(self.device)\n",
    "\n",
    "        self.d_optimizer.zero_grad()\n",
    "        real_output = self.discriminator(real_images)\n",
    "        d_real_loss = self.bce_loss(real_output, real_label)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake_images = self.generator(real_images, watermark)\n",
    "        fake_output = self.discriminator(fake_images.detach())\n",
    "        d_fake_loss = self.bce_loss(fake_output, fake_label)\n",
    "\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        self.d_optimizer.step()\n",
    "\n",
    "        return d_loss, {\n",
    "            'd_real_loss': d_real_loss.item(),\n",
    "            'd_fake_loss': d_fake_loss.item(),\n",
    "            'd_total_loss': d_loss.item()\n",
    "        }\n",
    "\n",
    "    def train_generator(\n",
    "        self,\n",
    "        real_images: torch.Tensor,\n",
    "        watermark: torch.Tensor\n",
    "    ) -> (torch.Tensor, dict):\n",
    "        batch_size = real_images.size(0)\n",
    "        real_label = torch.ones(batch_size, 1, 31, 31).to(self.device)\n",
    "\n",
    "        self.g_optimizer.zero_grad()\n",
    "        fake_images = self.generator(real_images, watermark)\n",
    "\n",
    "        mse_loss = self.mse_loss(fake_images, real_images)\n",
    "        fake_output = self.discriminator(fake_images)\n",
    "        adv_loss = self.bce_loss(fake_output, real_label)\n",
    "\n",
    "        g_loss = mse_loss + 0.1 * adv_loss\n",
    "        g_loss.backward()\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        return g_loss, {\n",
    "            'mse_loss': mse_loss.item(),\n",
    "            'adv_loss': adv_loss.item(),\n",
    "            'g_total_loss': g_loss.item()\n",
    "        }\n",
    "\n",
    "    def train_epoch(self) -> dict:\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        epoch_losses = {\n",
    "            'g_loss': 0.0,\n",
    "            'd_loss': 0.0,\n",
    "            'mse_loss': 0.0,\n",
    "            'adv_loss': 0.0\n",
    "        }\n",
    "\n",
    "        num_batches = len(self.train_loader)\n",
    "        progress_bar = tqdm(self.train_loader, desc=f'Epoch {self.current_epoch+1}')\n",
    "        for batch_idx, (real_images, watermark) in enumerate(progress_bar):\n",
    "            real_images = real_images.to(self.device)\n",
    "            watermark = watermark.to(self.device)\n",
    "\n",
    "            d_loss, d_losses = self.train_discriminator(real_images, watermark)\n",
    "            g_loss, g_losses = self.train_generator(real_images, watermark)\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'D_loss': f\"{d_loss.item():.4f}\",\n",
    "                'G_loss': f\"{g_loss.item():.4f}\"\n",
    "            })\n",
    "\n",
    "            epoch_losses['d_loss'] += d_loss.item()\n",
    "            epoch_losses['g_loss'] += g_loss.item()\n",
    "            epoch_losses['mse_loss'] += g_losses['mse_loss']\n",
    "            epoch_losses['adv_loss'] += g_losses['adv_loss']\n",
    "\n",
    "        for key in epoch_losses:\n",
    "            epoch_losses[key] /= num_batches\n",
    "            self.train_history[key].append(epoch_losses[key])\n",
    "\n",
    "        self.current_epoch += 1\n",
    "        return epoch_losses\n",
    "\n",
    "    def save_checkpoint(self, save_path: str, epoch: int):\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': self.generator.state_dict(),\n",
    "            'discriminator_state_dict': self.discriminator.state_dict(),\n",
    "            'g_optimizer_state_dict': self.g_optimizer.state_dict(),\n",
    "            'd_optimizer_state_dict': self.d_optimizer.state_dict(),\n",
    "            'train_history': self.train_history\n",
    "        }, os.path.join(save_path, f'checkpoint_epoch_{epoch}.pt'))\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path: str):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
    "        self.d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
    "        self.current_epoch = checkpoint['epoch']\n",
    "        self.train_history = checkpoint['train_history']\n",
    "\n",
    "    def save_sample_images(self, sample_loader, epoch: int, save_dir: str = 'samples', save_side_by_side: bool = False):\n",
    "        \"\"\"\n",
    "        Saves every watermarked image from the sample loader.\n",
    "        If save_side_by_side is True, saves a side-by-side image of the original and watermarked image.\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.generator.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, watermarks) in enumerate(sample_loader):\n",
    "                images = images.to(self.device)\n",
    "                watermarks = watermarks.to(self.device)\n",
    "                outputs = self.generator(images, watermarks)\n",
    "                for img_idx in range(outputs.size(0)):\n",
    "                    if save_side_by_side:\n",
    "                        # Concatenate original and watermarked images side by side (along width)\n",
    "                        combined = torch.cat((images[img_idx], outputs[img_idx]), dim=2)\n",
    "                        save_path = os.path.join(save_dir, f'epoch_{epoch}_batch_{batch_idx}_image_{img_idx}_combined.png')\n",
    "                        vutils.save_image(combined, save_path, normalize=True)\n",
    "                        # print(f\"Saved side-by-side image to {save_path}\")\n",
    "                    else:\n",
    "                        save_path = os.path.join(save_dir, f'epoch_{epoch}_batch_{batch_idx}_image_{img_idx}.png')\n",
    "                        vutils.save_image(outputs[img_idx], save_path, normalize=True)\n",
    "                        # print(f\"Saved image to {save_path}\")\n",
    "        self.generator.train()\n",
    "\n",
    "    def train(self, num_epochs: int, save_path: str = None, sample_loader=None, start_epoch: int = 0) -> dict:\n",
    "        \"\"\"\n",
    "        Trains for num_epochs starting from start_epoch.\n",
    "        If sample_loader is provided, sample images are saved every epoch.\n",
    "        \"\"\"\n",
    "        print(f\"Starting training from epoch {start_epoch+1} for {num_epochs} epochs...\")\n",
    "        start_time = time.time()\n",
    "        self.current_epoch = start_epoch  # resume from specified epoch if checkpoint loaded\n",
    "\n",
    "        for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "            epoch_losses = self.train_epoch()\n",
    "            print(f\"\\nEpoch {self.current_epoch} Summary:\")\n",
    "            for key, value in epoch_losses.items():\n",
    "                print(f\"{key}: {value:.4f}\")\n",
    "            if save_path and (epoch + 1) % 1 == 0:\n",
    "                self.save_checkpoint(save_path, epoch + 1)\n",
    "            if sample_loader is not None:\n",
    "                # Set save_side_by_side to True if you want original and watermarked images together\n",
    "                self.save_sample_images(sample_loader, epoch + 1, save_dir='sample_watermarks', save_side_by_side=False)\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {total_time/60:.2f} minutes\")\n",
    "        return self.train_history\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Helper functions for DataLoader and Setup\n",
    "# ------------------------------\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "\n",
    "def create_watermark_dataloaders(preprocessor, batch_size: int = 8, limit_images: int = None) -> (DataLoader, DataLoader):\n",
    "\n",
    "    train_dataset = WatermarkMRIDataset(preprocessor.train_df)\n",
    "    test_dataset = WatermarkMRIDataset(preprocessor.test_df)\n",
    "\n",
    "    # Limit the dataset to the first 'limit_images' samples if specified\n",
    "    if limit_images is not None:\n",
    "        train_dataset = Subset(train_dataset, range(limit_images))\n",
    "        test_dataset = Subset(test_dataset, range(limit_images))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def setup_training(dataset_path: str, batch_size: int = 8, limit_images: int = 100):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    preprocessor = MRIDatasetPreprocessor(dataset_path)\n",
    "    train_df, test_df = preprocessor.create_dataset_dataframes()\n",
    "    #\n",
    "    train_loader, test_loader = create_watermark_dataloaders(preprocessor, batch_size)\n",
    "\n",
    "    #\n",
    "    maxvit = MaxViT(in_channels=768, embed_dim=768, depth=4).to(device)\n",
    "    generator = UNetGenerator(maxvit).to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "\n",
    "    trainer = WatermarkTrainer(generator, discriminator, train_loader, test_loader, device)\n",
    "    return trainer, preprocessor, test_loader\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = path\n",
    "\n",
    "    trainer, preprocessor, test_loader = setup_training(dataset_path, batch_size=8)\n",
    "\n",
    "    history = trainer.train(num_epochs=10, save_path='checkpoints', sample_loader=test_loader)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Processing training set...\n",
      "Processing testing set...\n",
      "\n",
      "Analyzing training set dimensions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing images: 100%|██████████| 5712/5712 [00:01<00:00, 3404.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing testing set dimensions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing images: 100%|██████████| 1311/1311 [00:00<00:00, 3258.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing training set for watermarking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing for watermarking: 100%|██████████| 5712/5712 [00:01<00:00, 5029.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing testing set for watermarking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing for watermarking: 100%|██████████| 1311/1311 [00:00<00:00, 4743.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from epoch 1 for 10 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 6/714 [02:31<4:58:21, 25.28s/it, D_loss=0.7079, G_loss=0.2126]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 268\u001B[39m\n\u001B[32m    264\u001B[39m dataset_path = path\n\u001B[32m    266\u001B[39m trainer, preprocessor, test_loader = setup_training(dataset_path, batch_size=\u001B[32m8\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m268\u001B[39m history = \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_path\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mcheckpoints\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 195\u001B[39m, in \u001B[36mWatermarkTrainer.train\u001B[39m\u001B[34m(self, num_epochs, save_path, sample_loader, start_epoch)\u001B[39m\n\u001B[32m    192\u001B[39m \u001B[38;5;28mself\u001B[39m.current_epoch = start_epoch  \u001B[38;5;66;03m# resume from specified epoch if checkpoint loaded\u001B[39;00m\n\u001B[32m    194\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(start_epoch, start_epoch + num_epochs):\n\u001B[32m--> \u001B[39m\u001B[32m195\u001B[39m     epoch_losses = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    196\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.current_epoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m Summary:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    197\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m epoch_losses.items():\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 120\u001B[39m, in \u001B[36mWatermarkTrainer.train_epoch\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    117\u001B[39m real_images = real_images.to(\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m    118\u001B[39m watermark = watermark.to(\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m d_loss, d_losses = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtrain_discriminator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreal_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwatermark\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    121\u001B[39m g_loss, g_losses = \u001B[38;5;28mself\u001B[39m.train_generator(real_images, watermark)\n\u001B[32m    123\u001B[39m progress_bar.set_postfix({\n\u001B[32m    124\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mD_loss\u001B[39m\u001B[33m'\u001B[39m: \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00md_loss.item()\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    125\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mG_loss\u001B[39m\u001B[33m'\u001B[39m: \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mg_loss.item()\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    126\u001B[39m })\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 69\u001B[39m, in \u001B[36mWatermarkTrainer.train_discriminator\u001B[39m\u001B[34m(self, real_images, watermark)\u001B[39m\n\u001B[32m     66\u001B[39m d_fake_loss = \u001B[38;5;28mself\u001B[39m.bce_loss(fake_output, fake_label)\n\u001B[32m     68\u001B[39m d_loss = (d_real_loss + d_fake_loss) / \u001B[32m2\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m69\u001B[39m \u001B[43md_loss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     70\u001B[39m \u001B[38;5;28mself\u001B[39m.d_optimizer.step()\n\u001B[32m     72\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m d_loss, {\n\u001B[32m     73\u001B[39m     \u001B[33m'\u001B[39m\u001B[33md_real_loss\u001B[39m\u001B[33m'\u001B[39m: d_real_loss.item(),\n\u001B[32m     74\u001B[39m     \u001B[33m'\u001B[39m\u001B[33md_fake_loss\u001B[39m\u001B[33m'\u001B[39m: d_fake_loss.item(),\n\u001B[32m     75\u001B[39m     \u001B[33m'\u001B[39m\u001B[33md_total_loss\u001B[39m\u001B[33m'\u001B[39m: d_loss.item()\n\u001B[32m     76\u001B[39m }\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:626\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    616\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    617\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    618\u001B[39m         Tensor.backward,\n\u001B[32m    619\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    624\u001B[39m         inputs=inputs,\n\u001B[32m    625\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m626\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    628\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    342\u001B[39m     retain_graph = create_graph\n\u001B[32m    344\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    821\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    822\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m823\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    824\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    825\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    826\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    827\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "# !zip -r ganimgs.zip sample_watermarks/"
   ],
   "metadata": {
    "id": "Pb5vLZM5W5KS"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import files\n",
    "# files.download('ganimgs.zip')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0Okk8A4YEZtU",
    "outputId": "d9e46745-0927-4674-fc28-23577415c019"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "I10k_9xgDYAB"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wf__qNCUEzKK",
    "outputId": "95c9ec8d-b466-4f08-e361-7edf0085efbe"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# !cp /content/drive/MyDrive/ganimgs.zip /content/"
   ],
   "metadata": {
    "id": "-EDSa7lGE1Jb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# !du -sh ganimgs.zip\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_bzq_CSFjti",
    "outputId": "d7421e39-f428-4bdf-8c9a-8064fad14bfb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "9aKiWs_NGF7C"
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
