{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYVN3V52LTwc",
    "outputId": "2ead09bc-325d-4ee7-ac82-4ba682e75676",
    "ExecuteTime": {
     "end_time": "2025-02-27T09:07:01.190800Z",
     "start_time": "2025-02-27T09:07:01.189142Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Epxp6vHdAdIR",
    "outputId": "8d25795d-ebeb-4f46-e28e-add5d3b9b4b0",
    "ExecuteTime": {
     "end_time": "2025-02-27T09:07:01.830976Z",
     "start_time": "2025-02-27T09:07:01.193162Z"
    }
   },
   "source": [
    "import kagglehub\n",
    "\n",
    "path = \"D:\\\\Dropbox\\\\UMA Augusta\\\\PhD\\\\Research Thesis\\\\brain_tumor_mri_dataset\"\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyrTZ5_2_ZqJ",
    "outputId": "3d61fe38-b5b5-471b-e97a-566b6706600b",
    "ExecuteTime": {
     "end_time": "2025-02-27T09:07:52.541912Z",
     "start_time": "2025-02-27T09:07:01.963976Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import hashlib\n",
    "from typing import Tuple, Dict, List\n",
    "import multiprocessing\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "class WatermarkMRIDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, image_size: int = 512, watermark_size: int = 64):\n",
    "        self.filepaths = dataframe['filepath'].values\n",
    "        self.image_hashes = dataframe['image_hash'].values\n",
    "        self.image_size = image_size\n",
    "        self.watermark_size = watermark_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def generate_watermark(self, image_hash: str) -> torch.Tensor:\n",
    "        #  hash to numpy array\n",
    "        hash_bytes = bytes.fromhex(image_hash)\n",
    "        hash_array = np.frombuffer(hash_bytes, dtype=np.uint8)\n",
    "        hash_array = hash_array.astype(np.float32) / 255.0\n",
    "\n",
    "        required_size = 16 * 32 * 32\n",
    "        if hash_array.size < required_size:\n",
    "            # Tile the hash_array until we have enough elements\n",
    "            reps = required_size // hash_array.size + 1\n",
    "            hash_array = np.tile(hash_array, reps)\n",
    "\n",
    "        watermark = hash_array[:required_size].reshape(16, 32, 32)\n",
    "        watermark_tensor = torch.from_numpy(watermark)\n",
    "\n",
    "        # Upsample\n",
    "        watermark_tensor = watermark_tensor.unsqueeze(0)  # Add batch dimension\n",
    "        watermark_tensor = F.interpolate(\n",
    "            watermark_tensor,\n",
    "            size=(self.watermark_size, self.watermark_size),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        watermark_tensor = watermark_tensor.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        return watermark_tensor\n",
    "\n",
    "\n",
    "    def process_image(self, image_path: str) -> torch.Tensor:\n",
    "        \"\"\"Load and process image to tensor.\"\"\"\n",
    "        img = Image.open(image_path)\n",
    "\n",
    "        # Convert to RGB if grayscale\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        # Resize\n",
    "        img = img.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
    "\n",
    "        # Convert to tensor\n",
    "        img_tensor = torch.from_numpy(np.array(img)).float().div(255.0)\n",
    "        img_tensor = img_tensor.permute(2, 0, 1)  # Convert to (C,H,W)\n",
    "\n",
    "        return img_tensor\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        img_tensor = self.process_image(self.filepaths[idx])\n",
    "\n",
    "        watermark_tensor = self.generate_watermark(self.image_hashes[idx])\n",
    "\n",
    "        return img_tensor, watermark_tensor\n",
    "\n",
    "class MRIDatasetPreprocessor:\n",
    "    def __init__(self, base_path: str):\n",
    "        # GPU\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Set up paths\n",
    "        self.base_path = Path(base_path)\n",
    "        self.train_path = self.base_path / 'Training'\n",
    "        self.test_path = self.base_path / 'Testing'\n",
    "\n",
    "        #  directories\n",
    "        self.processed_dir = Path('processed_data')\n",
    "        self.watermarks_dir = Path('watermarks')\n",
    "        self.watermarked_dir = Path('watermarked_images')\n",
    "\n",
    "        for dir_path in [self.processed_dir, self.watermarks_dir, self.watermarked_dir]:\n",
    "            dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.image_stats = {}\n",
    "        self.watermark_stats = {}\n",
    "\n",
    "    def create_dataloaders(self, batch_size: int = 32) -> Tuple[DataLoader, DataLoader]:\n",
    "\n",
    "        train_dataset = WatermarkMRIDataset(self.train_df)\n",
    "        test_dataset = WatermarkMRIDataset(self.test_df)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "\n",
    "        return train_loader, test_loader\n",
    "\n",
    "    def pad_to_multiple_32(self, image: torch.Tensor) -> Tuple[torch.Tensor, Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Pad image to multiple of 32 while preserving dimensions info\n",
    "        \"\"\"\n",
    "        _, h, w = image.shape\n",
    "        pad_h = (32 - h % 32) % 32\n",
    "        pad_w = (32 - w % 32) % 32\n",
    "\n",
    "        padding = (0, pad_w, 0, pad_h)  # left, right, top, bottom\n",
    "        padded_image = F.pad(image, padding, mode='reflect')\n",
    "\n",
    "        return padded_image, (pad_h, pad_w)\n",
    "\n",
    "    def analyze_image_dimensions(self, dataframe: pd.DataFrame) -> Dict:\n",
    "\n",
    "        dimensions = []\n",
    "        sizes_kb = []\n",
    "        aspect_ratios = []\n",
    "\n",
    "        for filepath in tqdm(dataframe['filepath'], desc=\"Analyzing images\"):\n",
    "            with Image.open(filepath) as img:\n",
    "                w, h = img.size\n",
    "                dimensions.append((w, h))\n",
    "                sizes_kb.append(os.path.getsize(filepath) / 1024)\n",
    "                aspect_ratios.append(w / h)\n",
    "\n",
    "        dimensions = np.array(dimensions)\n",
    "        sizes_kb = np.array(sizes_kb)\n",
    "\n",
    "        stats = {\n",
    "            'unique_dimensions': np.unique(dimensions, axis=0),\n",
    "            'min_width': dimensions[:, 0].min(),\n",
    "            'max_width': dimensions[:, 0].max(),\n",
    "            'min_height': dimensions[:, 1].min(),\n",
    "            'max_height': dimensions[:, 1].max(),\n",
    "            'mean_width': dimensions[:, 0].mean(),\n",
    "            'mean_height': dimensions[:, 1].mean(),\n",
    "            'min_size_kb': sizes_kb.min(),\n",
    "            'max_size_kb': sizes_kb.max(),\n",
    "            'mean_size_kb': sizes_kb.mean(),\n",
    "            'aspect_ratios': aspect_ratios,\n",
    "            'min_aspect_ratio': min(aspect_ratios),\n",
    "            'max_aspect_ratio': max(aspect_ratios),\n",
    "            'mean_aspect_ratio': np.mean(aspect_ratios)\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def analyze_for_watermarking(self, dataframe: pd.DataFrame) -> Dict:\n",
    "        watermark_stats = {\n",
    "            'min_dimension': float('inf'),\n",
    "            'max_dimension': 0,\n",
    "            'aspect_ratios': [],\n",
    "            'optimal_watermark_sizes': []\n",
    "        }\n",
    "\n",
    "        for filepath in tqdm(dataframe['filepath'], desc=\"Analyzing for watermarking\"):\n",
    "            with Image.open(filepath) as img:\n",
    "                w, h = img.size\n",
    "                min_dim = min(w, h)\n",
    "                max_dim = max(w, h)\n",
    "                aspect_ratio = w / h\n",
    "\n",
    "                watermark_stats['min_dimension'] = min(watermark_stats['min_dimension'], min_dim)\n",
    "                watermark_stats['max_dimension'] = max(watermark_stats['max_dimension'], max_dim)\n",
    "                watermark_stats['aspect_ratios'].append(aspect_ratio)\n",
    "\n",
    "                # Calculate optimal watermark size for this image\n",
    "                optimal_size = max(32, min_dim // 8)\n",
    "                watermark_stats['optimal_watermark_sizes'].append(optimal_size)\n",
    "\n",
    "        watermark_stats['mean_optimal_size'] = np.mean(watermark_stats['optimal_watermark_sizes'])\n",
    "        watermark_stats['median_optimal_size'] = np.median(watermark_stats['optimal_watermark_sizes'])\n",
    "\n",
    "        return watermark_stats\n",
    "\n",
    "    def create_dataset_dataframes(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "        def process_directory(base_path: Path) -> pd.DataFrame:\n",
    "            filepaths = []\n",
    "            labels = []\n",
    "            dimensions = []\n",
    "            image_hashes = []\n",
    "\n",
    "            for fold in os.listdir(base_path):\n",
    "                fold_path = base_path / fold\n",
    "                if not fold_path.is_dir():\n",
    "                    continue\n",
    "\n",
    "                for file in os.listdir(fold_path):\n",
    "                    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        filepath = str(fold_path / file)\n",
    "                        with Image.open(filepath) as img:\n",
    "                            dimensions.append(img.size)\n",
    "                            # Calculate image hash\n",
    "                            img_array = np.array(img)\n",
    "                            img_hash = hashlib.sha256(img_array.tobytes()).hexdigest()\n",
    "                            image_hashes.append(img_hash)\n",
    "\n",
    "                        filepaths.append(filepath)\n",
    "                        labels.append(fold)\n",
    "\n",
    "            return pd.DataFrame({\n",
    "                'filepath': filepaths,\n",
    "                'label': labels,\n",
    "                'dimensions': dimensions,\n",
    "                'image_hash': image_hashes\n",
    "            })\n",
    "\n",
    "        print(\"Processing training set...\")\n",
    "        self.train_df = process_directory(self.train_path)\n",
    "        print(\"Processing testing set...\")\n",
    "        self.test_df = process_directory(self.test_path)\n",
    "\n",
    "        print(\"\\nAnalyzing training set dimensions...\")\n",
    "        self.image_stats['train'] = self.analyze_image_dimensions(self.train_df)\n",
    "        print(\"\\nAnalyzing testing set dimensions...\")\n",
    "        self.image_stats['test'] = self.analyze_image_dimensions(self.test_df)\n",
    "\n",
    "        print(\"\\nAnalyzing training set for watermarking...\")\n",
    "        self.watermark_stats['train'] = self.analyze_for_watermarking(self.train_df)\n",
    "        print(\"\\nAnalyzing testing set for watermarking...\")\n",
    "        self.watermark_stats['test'] = self.analyze_for_watermarking(self.test_df)\n",
    "\n",
    "        return self.train_df, self.test_df\n",
    "\n",
    "    def save_dataset_info(self):\n",
    "        self.train_df.to_csv(self.processed_dir / 'train_info.csv', index=False)\n",
    "        self.test_df.to_csv(self.processed_dir / 'test_info.csv', index=False)\n",
    "\n",
    "        stats_df = pd.DataFrame({\n",
    "            'train': self.image_stats['train'],\n",
    "            'test': self.image_stats['test']\n",
    "        })\n",
    "        stats_df.to_csv(self.processed_dir / 'image_statistics.csv')\n",
    "\n",
    "        watermark_stats_df = pd.DataFrame({\n",
    "            'train': self.watermark_stats['train'],\n",
    "            'test': self.watermark_stats['test']\n",
    "        })\n",
    "        watermark_stats_df.to_csv(self.processed_dir / 'watermark_statistics.csv')\n",
    "\n",
    "    def get_dataset_statistics(self):\n",
    "\n",
    "        print(\"\\n=== Dataset Statistics Summary ===\")\n",
    "        print(f\"Total training images: {len(self.train_df)}\")\n",
    "        print(f\"Total testing images: {len(self.test_df)}\")\n",
    "\n",
    "        for dataset_type in ['train', 'test']:\n",
    "            stats = self.image_stats[dataset_type]\n",
    "            wm_stats = self.watermark_stats[dataset_type]\n",
    "\n",
    "            print(f\"\\n{dataset_type.capitalize()} Set Summary:\")\n",
    "            print(f\"Dimensions:\")\n",
    "            print(f\"  Min Width: {stats['min_width']}, Max Width: {stats['max_width']}\")\n",
    "            print(f\"  Min Height: {stats['min_height']}, Max Height: {stats['max_height']}\")\n",
    "            print(f\"  Mean Width: {stats['mean_width']:.2f}, Mean Height: {stats['mean_height']:.2f}\")\n",
    "\n",
    "            print(f\"\\nFile Sizes:\")\n",
    "            print(f\"  Min: {stats['min_size_kb']:.2f} KB\")\n",
    "            print(f\"  Max: {stats['max_size_kb']:.2f} KB\")\n",
    "            print(f\"  Mean: {stats['mean_size_kb']:.2f} KB\")\n",
    "\n",
    "            print(f\"\\nWatermarking Info:\")\n",
    "            print(f\"  Min Dimension: {wm_stats['min_dimension']}\")\n",
    "            print(f\"  Max Dimension: {wm_stats['max_dimension']}\")\n",
    "            print(f\"  Mean Optimal Watermark Size: {wm_stats['mean_optimal_size']:.2f}\")\n",
    "            print(f\"  Median Optimal Watermark Size: {wm_stats['median_optimal_size']:.2f}\")\n",
    "\n",
    "def test_preprocessor(dataset_path: str):\n",
    "    try:\n",
    "        print(\"Initializing preprocessor...\")\n",
    "        preprocessor = MRIDatasetPreprocessor(dataset_path)\n",
    "\n",
    "        print(\"\\nCreating and analyzing datasets...\")\n",
    "        train_df, test_df = preprocessor.create_dataset_dataframes()\n",
    "\n",
    "        print(\"\\nCreating dataloaders...\")\n",
    "        train_loader, test_loader = preprocessor.create_dataloaders(batch_size=3)\n",
    "\n",
    "        print(\"\\nSaving dataset information...\")\n",
    "        preprocessor.save_dataset_info()\n",
    "\n",
    "        print(\"\\nDisplaying dataset statistics...\")\n",
    "        preprocessor.get_dataset_statistics()\n",
    "\n",
    "        # Test a batch\n",
    "        # print(\"\\nTesting batch loading...\")\n",
    "        # for images, watermarks in train_loader:\n",
    "        #     images = images.to(preprocessor.device)\n",
    "        #     watermarks = watermarks.to(preprocessor.device)\n",
    "        #     print(f\"Image batch shape: {images.shape}\")\n",
    "        #     print(f\"Watermark batch shape: {watermarks.shape}\")\n",
    "        #     print(f\"Image value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "        #     print(f\"Watermark value range: [{watermarks.min():.3f}, {watermarks.max():.3f}]\")\n",
    "        #     break  # Only test first batch\n",
    "\n",
    "        return preprocessor, train_loader, test_loader\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = path\n",
    "    preprocessor, train_loader, test_loader = test_preprocessor(dataset_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing preprocessor...\n",
      "Using device: cuda\n",
      "\n",
      "Creating and analyzing datasets...\n",
      "Processing training set...\n",
      "Processing testing set...\n",
      "\n",
      "Analyzing training set dimensions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing images: 100%|██████████| 5712/5712 [00:11<00:00, 484.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing testing set dimensions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing images: 100%|██████████| 1311/1311 [00:02<00:00, 498.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing training set for watermarking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing for watermarking: 100%|██████████| 5712/5712 [00:11<00:00, 493.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing testing set for watermarking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing for watermarking: 100%|██████████| 1311/1311 [00:02<00:00, 498.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating dataloaders...\n",
      "\n",
      "Saving dataset information...\n",
      "\n",
      "Displaying dataset statistics...\n",
      "\n",
      "=== Dataset Statistics Summary ===\n",
      "Total training images: 5712\n",
      "Total testing images: 1311\n",
      "\n",
      "Train Set Summary:\n",
      "Dimensions:\n",
      "  Min Width: 150, Max Width: 1920\n",
      "  Min Height: 168, Max Height: 1446\n",
      "  Mean Width: 451.56, Mean Height: 453.88\n",
      "\n",
      "File Sizes:\n",
      "  Min: 3.39 KB\n",
      "  Max: 710.85 KB\n",
      "  Mean: 22.64 KB\n",
      "\n",
      "Watermarking Info:\n",
      "  Min Dimension: 150\n",
      "  Max Dimension: 1920\n",
      "  Mean Optimal Watermark Size: 56.96\n",
      "  Median Optimal Watermark Size: 64.00\n",
      "\n",
      "Test Set Summary:\n",
      "Dimensions:\n",
      "  Min Width: 150, Max Width: 1149\n",
      "  Min Height: 168, Max Height: 1019\n",
      "  Mean Width: 421.18, Mean Height: 424.22\n",
      "\n",
      "File Sizes:\n",
      "  Min: 4.58 KB\n",
      "  Max: 118.71 KB\n",
      "  Mean: 19.48 KB\n",
      "\n",
      "Watermarking Info:\n",
      "  Min Dimension: 150\n",
      "  Max Dimension: 1149\n",
      "  Mean Optimal Watermark Size: 53.42\n",
      "  Median Optimal Watermark Size: 64.00\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVOVozNI_cwc",
    "outputId": "96b7e2a9-811d-462a-9fd0-cf911d4dd5a1",
    "ExecuteTime": {
     "end_time": "2025-02-27T09:07:53.355552Z",
     "start_time": "2025-02-27T09:07:52.556540Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsummary\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int = 8, qkv_bias: bool = False, attn_drop: float = 0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class MaxViTBlock(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 4., qkv_bias: bool = False,\n",
    "                 drop: float = 0., attn_drop: float = 0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class MaxViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 512,\n",
    "                 embed_dim: int = 768,\n",
    "                 depth: int = 16,\n",
    "                 num_heads: int = 8,\n",
    "                 mlp_ratio: float = 4.,\n",
    "                 qkv_bias: bool = False,\n",
    "                 drop_rate: float = 0.,\n",
    "                 attn_drop_rate: float = 0.):\n",
    "        \"\"\"\n",
    "        Initialize MaxViT model.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial convolution to change channel dimensions\n",
    "        self.conv_in = nn.Conv2d(in_channels, embed_dim, kernel_size=1)\n",
    "\n",
    "        # Position embedding\n",
    "        self.feature_size = 32 * 32  # Fixed size for bottleneck\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.feature_size, embed_dim))\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MaxViTBlock(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Final projection back to input channels\n",
    "        self.conv_out = nn.Conv2d(embed_dim, in_channels, kernel_size=1)\n",
    "\n",
    "        # Initialize position embedding\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        assert H == W == int(self.feature_size ** 0.5), \\\n",
    "            f\"Input spatial dimensions must be {int(self.feature_size ** 0.5)}x{int(self.feature_size ** 0.5)}, got {H}x{W}\"\n",
    "\n",
    "        # Initial convolution: (B, C, H, W) -> (B, embed_dim, H, W)\n",
    "        x = self.conv_in(x)\n",
    "\n",
    "        # Reshape for transformer: (B, embed_dim, H, W) -> (B, H*W, embed_dim)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # Position embedding\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Reshape back: (B, H*W, embed_dim) -> (B, embed_dim, H, W)\n",
    "        x = x.transpose(1, 2).reshape(B, -1, H, W)\n",
    "\n",
    "        # Final projection: (B, embed_dim, H, W) -> (B, C, H, W)\n",
    "        x = self.conv_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class WatermarkProcessor(nn.Module):\n",
    "    def __init__(self, in_channels: int = 16):\n",
    "        super().__init__()\n",
    "        self.process = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 256, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.process(x)\n",
    "\n",
    "def test_maxvit():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    x = torch.randn(2, 512, 32, 32).to(device)\n",
    "    print(f\"Created input tensor with shape: {x.shape}\")\n",
    "\n",
    "    model = MaxViT(in_channels=512, embed_dim=768, depth=16).to(device)\n",
    "    print(\"Initialized MaxViT model\")\n",
    "\n",
    "    torchsummary.summary(model, input_size=(512, 32, 32))\n",
    "\n",
    "    # Save Model Summary to File\n",
    "    with open(\"maxvit_model_summary.txt\", \"w\") as f:\n",
    "        import sys\n",
    "        sys.stdout = f\n",
    "        torchsummary.summary(model, input_size=(512, 32, 32))\n",
    "        sys.stdout = sys.__stdout__\n",
    "\n",
    "    print(\"\\nPerforming forward pass...\")\n",
    "    output = model(x)\n",
    "\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Running on: {device}\")\n",
    "\n",
    "    if x.shape == output.shape:\n",
    "        print(\"OK: Input and output shapes match\")\n",
    "    else:\n",
    "        print(\"FAIL: Input and output shapes differ!\")\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_maxvit()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Created input tensor with shape: torch.Size([2, 512, 32, 32])\n",
      "Initialized MaxViT model\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 32, 32]         393,984\n",
      "         LayerNorm-2            [-1, 1024, 768]           1,536\n",
      "            Linear-3           [-1, 1024, 2304]       1,769,472\n",
      "           Dropout-4        [-1, 8, 1024, 1024]               0\n",
      "            Linear-5            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-6            [-1, 1024, 768]               0\n",
      "         LayerNorm-7            [-1, 1024, 768]           1,536\n",
      "            Linear-8           [-1, 1024, 3072]       2,362,368\n",
      "              GELU-9           [-1, 1024, 3072]               0\n",
      "          Dropout-10           [-1, 1024, 3072]               0\n",
      "           Linear-11            [-1, 1024, 768]       2,360,064\n",
      "          Dropout-12            [-1, 1024, 768]               0\n",
      "      MaxViTBlock-13            [-1, 1024, 768]               0\n",
      "        LayerNorm-14            [-1, 1024, 768]           1,536\n",
      "           Linear-15           [-1, 1024, 2304]       1,769,472\n",
      "          Dropout-16        [-1, 8, 1024, 1024]               0\n",
      "           Linear-17            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-18            [-1, 1024, 768]               0\n",
      "        LayerNorm-19            [-1, 1024, 768]           1,536\n",
      "           Linear-20           [-1, 1024, 3072]       2,362,368\n",
      "             GELU-21           [-1, 1024, 3072]               0\n",
      "          Dropout-22           [-1, 1024, 3072]               0\n",
      "           Linear-23            [-1, 1024, 768]       2,360,064\n",
      "          Dropout-24            [-1, 1024, 768]               0\n",
      "      MaxViTBlock-25            [-1, 1024, 768]               0\n",
      "        LayerNorm-26            [-1, 1024, 768]           1,536\n",
      "           Linear-27           [-1, 1024, 2304]       1,769,472\n",
      "          Dropout-28        [-1, 8, 1024, 1024]               0\n",
      "           Linear-29            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-30            [-1, 1024, 768]               0\n",
      "        LayerNorm-31            [-1, 1024, 768]           1,536\n",
      "           Linear-32           [-1, 1024, 3072]       2,362,368\n",
      "             GELU-33           [-1, 1024, 3072]               0\n",
      "          Dropout-34           [-1, 1024, 3072]               0\n",
      "           Linear-35            [-1, 1024, 768]       2,360,064\n",
      "          Dropout-36            [-1, 1024, 768]               0\n",
      "      MaxViTBlock-37            [-1, 1024, 768]               0\n",
      "        LayerNorm-38            [-1, 1024, 768]           1,536\n",
      "           Linear-39           [-1, 1024, 2304]       1,769,472\n",
      "          Dropout-40        [-1, 8, 1024, 1024]               0\n",
      "           Linear-41            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-42            [-1, 1024, 768]               0\n",
      "        LayerNorm-43            [-1, 1024, 768]           1,536\n",
      "           Linear-44           [-1, 1024, 3072]       2,362,368\n",
      "             GELU-45           [-1, 1024, 3072]               0\n",
      "          Dropout-46           [-1, 1024, 3072]               0\n",
      "           Linear-47            [-1, 1024, 768]       2,360,064\n",
      "          Dropout-48            [-1, 1024, 768]               0\n",
      "      MaxViTBlock-49            [-1, 1024, 768]               0\n",
      "        LayerNorm-50            [-1, 1024, 768]           1,536\n",
      "           Linear-51           [-1, 1024, 2304]       1,769,472\n",
      "          Dropout-52        [-1, 8, 1024, 1024]               0\n",
      "           Linear-53            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-54            [-1, 1024, 768]               0\n",
      "        LayerNorm-55            [-1, 1024, 768]           1,536\n",
      "           Linear-56           [-1, 1024, 3072]       2,362,368\n",
      "             GELU-57           [-1, 1024, 3072]               0\n",
      "          Dropout-58           [-1, 1024, 3072]               0\n",
      "           Linear-59            [-1, 1024, 768]       2,360,064\n",
      "          Dropout-60            [-1, 1024, 768]               0\n",
      "      MaxViTBlock-61            [-1, 1024, 768]               0\n",
      "        LayerNorm-62            [-1, 1024, 768]           1,536\n",
      "           Linear-63           [-1, 1024, 2304]       1,769,472\n",
      "          Dropout-64        [-1, 8, 1024, 1024]               0\n",
      "           Linear-65            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-66            [-1, 1024, 768]               0\n",
      "        LayerNorm-67            [-1, 1024, 768]           1,536\n",
      "           Linear-68           [-1, 1024, 3072]       2,362,368\n",
      "             GELU-69           [-1, 1024, 3072]               0\n",
      "          Dropout-70           [-1, 1024, 3072]               0\n",
      "           Linear-71            [-1, 1024, 768]       2,360,064\n",
      "          Dropout-72            [-1, 1024, 768]               0\n",
      "      MaxViTBlock-73            [-1, 1024, 768]               0\n",
      "        LayerNorm-74            [-1, 1024, 768]           1,536\n",
      "           Linear-75           [-1, 1024, 2304]       1,769,472\n",
      "          Dropout-76        [-1, 8, 1024, 1024]               0\n",
      "           Linear-77            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-78            [-1, 1024, 768]               0\n",
      "        LayerNorm-79            [-1, 1024, 768]           1,536\n",
      "           Linear-80           [-1, 1024, 3072]       2,362,368\n",
      "             GELU-81           [-1, 1024, 3072]               0\n",
      "          Dropout-82           [-1, 1024, 3072]               0\n",
      "           Linear-83            [-1, 1024, 768]       2,360,064\n",
      "          Dropout-84            [-1, 1024, 768]               0\n",
      "      MaxViTBlock-85            [-1, 1024, 768]               0\n",
      "        LayerNorm-86            [-1, 1024, 768]           1,536\n",
      "           Linear-87           [-1, 1024, 2304]       1,769,472\n",
      "          Dropout-88        [-1, 8, 1024, 1024]               0\n",
      "           Linear-89            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-90            [-1, 1024, 768]               0\n",
      "        LayerNorm-91            [-1, 1024, 768]           1,536\n",
      "           Linear-92           [-1, 1024, 3072]       2,362,368\n",
      "             GELU-93           [-1, 1024, 3072]               0\n",
      "          Dropout-94           [-1, 1024, 3072]               0\n",
      "           Linear-95            [-1, 1024, 768]       2,360,064\n",
      "          Dropout-96            [-1, 1024, 768]               0\n",
      "      MaxViTBlock-97            [-1, 1024, 768]               0\n",
      "        LayerNorm-98            [-1, 1024, 768]           1,536\n",
      "           Linear-99           [-1, 1024, 2304]       1,769,472\n",
      "         Dropout-100        [-1, 8, 1024, 1024]               0\n",
      "          Linear-101            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-102            [-1, 1024, 768]               0\n",
      "       LayerNorm-103            [-1, 1024, 768]           1,536\n",
      "          Linear-104           [-1, 1024, 3072]       2,362,368\n",
      "            GELU-105           [-1, 1024, 3072]               0\n",
      "         Dropout-106           [-1, 1024, 3072]               0\n",
      "          Linear-107            [-1, 1024, 768]       2,360,064\n",
      "         Dropout-108            [-1, 1024, 768]               0\n",
      "     MaxViTBlock-109            [-1, 1024, 768]               0\n",
      "       LayerNorm-110            [-1, 1024, 768]           1,536\n",
      "          Linear-111           [-1, 1024, 2304]       1,769,472\n",
      "         Dropout-112        [-1, 8, 1024, 1024]               0\n",
      "          Linear-113            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-114            [-1, 1024, 768]               0\n",
      "       LayerNorm-115            [-1, 1024, 768]           1,536\n",
      "          Linear-116           [-1, 1024, 3072]       2,362,368\n",
      "            GELU-117           [-1, 1024, 3072]               0\n",
      "         Dropout-118           [-1, 1024, 3072]               0\n",
      "          Linear-119            [-1, 1024, 768]       2,360,064\n",
      "         Dropout-120            [-1, 1024, 768]               0\n",
      "     MaxViTBlock-121            [-1, 1024, 768]               0\n",
      "       LayerNorm-122            [-1, 1024, 768]           1,536\n",
      "          Linear-123           [-1, 1024, 2304]       1,769,472\n",
      "         Dropout-124        [-1, 8, 1024, 1024]               0\n",
      "          Linear-125            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-126            [-1, 1024, 768]               0\n",
      "       LayerNorm-127            [-1, 1024, 768]           1,536\n",
      "          Linear-128           [-1, 1024, 3072]       2,362,368\n",
      "            GELU-129           [-1, 1024, 3072]               0\n",
      "         Dropout-130           [-1, 1024, 3072]               0\n",
      "          Linear-131            [-1, 1024, 768]       2,360,064\n",
      "         Dropout-132            [-1, 1024, 768]               0\n",
      "     MaxViTBlock-133            [-1, 1024, 768]               0\n",
      "       LayerNorm-134            [-1, 1024, 768]           1,536\n",
      "          Linear-135           [-1, 1024, 2304]       1,769,472\n",
      "         Dropout-136        [-1, 8, 1024, 1024]               0\n",
      "          Linear-137            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-138            [-1, 1024, 768]               0\n",
      "       LayerNorm-139            [-1, 1024, 768]           1,536\n",
      "          Linear-140           [-1, 1024, 3072]       2,362,368\n",
      "            GELU-141           [-1, 1024, 3072]               0\n",
      "         Dropout-142           [-1, 1024, 3072]               0\n",
      "          Linear-143            [-1, 1024, 768]       2,360,064\n",
      "         Dropout-144            [-1, 1024, 768]               0\n",
      "     MaxViTBlock-145            [-1, 1024, 768]               0\n",
      "       LayerNorm-146            [-1, 1024, 768]           1,536\n",
      "          Linear-147           [-1, 1024, 2304]       1,769,472\n",
      "         Dropout-148        [-1, 8, 1024, 1024]               0\n",
      "          Linear-149            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-150            [-1, 1024, 768]               0\n",
      "       LayerNorm-151            [-1, 1024, 768]           1,536\n",
      "          Linear-152           [-1, 1024, 3072]       2,362,368\n",
      "            GELU-153           [-1, 1024, 3072]               0\n",
      "         Dropout-154           [-1, 1024, 3072]               0\n",
      "          Linear-155            [-1, 1024, 768]       2,360,064\n",
      "         Dropout-156            [-1, 1024, 768]               0\n",
      "     MaxViTBlock-157            [-1, 1024, 768]               0\n",
      "       LayerNorm-158            [-1, 1024, 768]           1,536\n",
      "          Linear-159           [-1, 1024, 2304]       1,769,472\n",
      "         Dropout-160        [-1, 8, 1024, 1024]               0\n",
      "          Linear-161            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-162            [-1, 1024, 768]               0\n",
      "       LayerNorm-163            [-1, 1024, 768]           1,536\n",
      "          Linear-164           [-1, 1024, 3072]       2,362,368\n",
      "            GELU-165           [-1, 1024, 3072]               0\n",
      "         Dropout-166           [-1, 1024, 3072]               0\n",
      "          Linear-167            [-1, 1024, 768]       2,360,064\n",
      "         Dropout-168            [-1, 1024, 768]               0\n",
      "     MaxViTBlock-169            [-1, 1024, 768]               0\n",
      "       LayerNorm-170            [-1, 1024, 768]           1,536\n",
      "          Linear-171           [-1, 1024, 2304]       1,769,472\n",
      "         Dropout-172        [-1, 8, 1024, 1024]               0\n",
      "          Linear-173            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-174            [-1, 1024, 768]               0\n",
      "       LayerNorm-175            [-1, 1024, 768]           1,536\n",
      "          Linear-176           [-1, 1024, 3072]       2,362,368\n",
      "            GELU-177           [-1, 1024, 3072]               0\n",
      "         Dropout-178           [-1, 1024, 3072]               0\n",
      "          Linear-179            [-1, 1024, 768]       2,360,064\n",
      "         Dropout-180            [-1, 1024, 768]               0\n",
      "     MaxViTBlock-181            [-1, 1024, 768]               0\n",
      "       LayerNorm-182            [-1, 1024, 768]           1,536\n",
      "          Linear-183           [-1, 1024, 2304]       1,769,472\n",
      "         Dropout-184        [-1, 8, 1024, 1024]               0\n",
      "          Linear-185            [-1, 1024, 768]         590,592\n",
      "MultiHeadAttention-186            [-1, 1024, 768]               0\n",
      "       LayerNorm-187            [-1, 1024, 768]           1,536\n",
      "          Linear-188           [-1, 1024, 3072]       2,362,368\n",
      "            GELU-189           [-1, 1024, 3072]               0\n",
      "         Dropout-190           [-1, 1024, 3072]               0\n",
      "          Linear-191            [-1, 1024, 768]       2,360,064\n",
      "         Dropout-192            [-1, 1024, 768]               0\n",
      "     MaxViTBlock-193            [-1, 1024, 768]               0\n",
      "          Conv2d-194          [-1, 512, 32, 32]         393,728\n",
      "================================================================\n",
      "Total params: 114,156,800\n",
      "Trainable params: 114,156,800\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.00\n",
      "Forward/backward pass size (MB): 3146.00\n",
      "Params size (MB): 435.47\n",
      "Estimated Total Size (MB): 3583.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XlsHOeITDG6X",
    "outputId": "f18c3a47-fe00-4569-c8b2-077ff6a6fad8",
    "ExecuteTime": {
     "end_time": "2025-02-27T09:07:54.044606Z",
     "start_time": "2025-02-27T09:07:53.360545Z"
    }
   },
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int = 8, qkv_bias: bool = False, attn_drop: float = 0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class MaxViTBlock(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 4., qkv_bias: bool = False,\n",
    "                 drop: float = 0., attn_drop: float = 0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class MaxViT(nn.Module):\n",
    "    def __init__(self, in_channels: int = 512, embed_dim: int = 768, depth: int = 16,\n",
    "                 num_heads: int = 8, mlp_ratio: float = 4., qkv_bias: bool = False,\n",
    "                 drop_rate: float = 0., attn_drop_rate: float = 0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, embed_dim, kernel_size=1)\n",
    "\n",
    "        self.feature_size = 32 * 32\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.feature_size, embed_dim))\n",
    "\n",
    "        # MaxViT blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MaxViTBlock(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.conv_out = nn.Conv2d(embed_dim, in_channels, kernel_size=1)\n",
    "\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        assert H * W == self.feature_size, f\"Input feature map size {H}x{W} doesn't match expected size 32x32\"\n",
    "\n",
    "        # Initial convolution\n",
    "        x = self.conv_in(x)  # Shape: (B, embed_dim, H, W)\n",
    "\n",
    "        x = x.flatten(2).transpose(1, 2)  # Shape: (B, H*W, embed_dim)\n",
    "\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, -1, H, W)\n",
    "\n",
    "        x = self.conv_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.gelu(self.bn1(self.conv1(x)))\n",
    "        x = self.gelu(self.bn2(self.conv2(x)))\n",
    "        skip = x\n",
    "        x = self.pool(x)\n",
    "        return x, skip\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, skip_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        # Upsampling\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels // 2 + skip_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.up(x)\n",
    "        # If spatial dimensions differ, pad appropriately.\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            diff_h = skip.size(2) - x.size(2)\n",
    "            diff_w = skip.size(3) - x.size(3)\n",
    "            x = F.pad(x, [diff_w // 2, diff_w - diff_w // 2,\n",
    "                          diff_h // 2, diff_h - diff_h // 2])\n",
    "        # Concatenate along the channel dimension.\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        x = self.gelu(self.bn1(self.conv1(x)))\n",
    "        x = self.gelu(self.bn2(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class AdaptiveWeightBlender(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.weight_conv = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, image: torch.Tensor, watermark: torch.Tensor) -> torch.Tensor:\n",
    "        assert image.shape == watermark.shape, f\"Shape mismatch: image {image.shape}, watermark {watermark.shape}\"\n",
    "\n",
    "        combined = torch.cat([image, watermark], dim=1)\n",
    "\n",
    "        weights = self.weight_conv(combined)\n",
    "\n",
    "        blended = image * weights + watermark * (1 - weights)\n",
    "        return blended\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, maxvit_model: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder pathway\n",
    "        self.enc1 = EncoderBlock(3, 64)      # 512x512 -> 256x256\n",
    "        self.enc2 = EncoderBlock(64, 128)    # 256x256 -> 128x128\n",
    "        self.enc3 = EncoderBlock(128, 256)   # 128x128 -> 64x64\n",
    "        self.enc4 = EncoderBlock(256, 512)   # 64x64 -> 32x32\n",
    "\n",
    "        # Watermark processor (16 -> 512 channels)\n",
    "        self.watermark_processor = nn.Sequential(\n",
    "            nn.Conv2d(16, 256, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.blender = AdaptiveWeightBlender(512)\n",
    "\n",
    "        self.pre_maxvit_conv = nn.Conv2d(512, 768, kernel_size=1)\n",
    "\n",
    "        self.maxvit = maxvit_model\n",
    "\n",
    "        self.post_maxvit_conv = nn.Conv2d(768, 512, kernel_size=1)\n",
    "\n",
    "        # Decoder pathway\n",
    "        self.dec1 = DecoderBlock(in_channels=512, skip_channels=512, out_channels=256)  # 32x32 -> 64x64\n",
    "        self.dec2 = DecoderBlock(in_channels=256, skip_channels=256, out_channels=128)  # 64x64 -> 128x128\n",
    "        self.dec3 = DecoderBlock(in_channels=128, skip_channels=128, out_channels=64)   # 128x128 -> 256x256\n",
    "\n",
    "\n",
    "        # Final output\n",
    "        self.final_conv = nn.Conv2d(64, 3, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        skip_connections = []\n",
    "\n",
    "        x, skip1 = self.enc1(x)\n",
    "        skip_connections.append(skip1)\n",
    "\n",
    "        x, skip2 = self.enc2(x)\n",
    "        skip_connections.append(skip2)\n",
    "\n",
    "        x, skip3 = self.enc3(x)\n",
    "        skip_connections.append(skip3)\n",
    "\n",
    "        x, skip4 = self.enc4(x)\n",
    "        skip_connections.append(skip4)\n",
    "\n",
    "        return x, skip_connections\n",
    "\n",
    "    def bottleneck(self, image_features: torch.Tensor, watermark: torch.Tensor) -> torch.Tensor:\n",
    "        # Process watermark (16 -> 512 channels)\n",
    "        watermark_features = self.watermark_processor(watermark)\n",
    "\n",
    "        # Resize watermark features if needed\n",
    "        if watermark_features.shape[2:] != image_features.shape[2:]:\n",
    "            watermark_features = F.interpolate(\n",
    "                watermark_features,\n",
    "                size=image_features.shape[2:],\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "        blended = self.blender(image_features, watermark_features)\n",
    "\n",
    "        maxvit_input = self.pre_maxvit_conv(blended)\n",
    "\n",
    "        maxvit_output = self.maxvit(maxvit_input)\n",
    "\n",
    "        refined = self.post_maxvit_conv(maxvit_output)\n",
    "\n",
    "        return refined\n",
    "\n",
    "    def decode(self, x: torch.Tensor, skip_connections: List[torch.Tensor]) -> torch.Tensor:\n",
    "        x = self.dec1(x, skip_connections[3])\n",
    "        x = self.dec2(x, skip_connections[2])\n",
    "        x = self.dec3(x, skip_connections[1])\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, image: torch.Tensor, watermark: torch.Tensor) -> torch.Tensor:\n",
    "        encoded, skip_connections = self.encode(image)\n",
    "        bottleneck_features = self.bottleneck(encoded, watermark)\n",
    "        decoded = self.decode(bottleneck_features, skip_connections)  # currently 256×256 output\n",
    "        # Upsample decoded output from 256x256 to 512x512\n",
    "        output = torch.nn.functional.interpolate(decoded, size=(512, 512), mode='bilinear', align_corners=False)\n",
    "        return output\n",
    "\n",
    "\n",
    "def test_generator():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    maxvit = MaxViT(\n",
    "        in_channels=768,\n",
    "        embed_dim=768,\n",
    "        depth=16\n",
    "    ).to(device)\n",
    "\n",
    "    generator = UNetGenerator(maxvit).to(device)\n",
    "\n",
    "    batch_size = 3\n",
    "\n",
    "    image = torch.randn(batch_size, 3, 512, 512).to(device)\n",
    "    watermark = torch.randn(batch_size, 16, 64, 64).to(device)\n",
    "\n",
    "    torchsummary.summary(generator, [(3, 512, 512), (16, 64, 64)])\n",
    "\n",
    "    # Save Model Summary to File\n",
    "    with open(\"unet_generator_summary.txt\", \"w\") as f:\n",
    "        import sys\n",
    "        sys.stdout = f\n",
    "        torchsummary.summary(generator, [(3, 512, 512), (16, 64, 64)])\n",
    "        sys.stdout = sys.__stdout__\n",
    "\n",
    "    print(\"\\nModel summary saved to unet_generator_summary.txt\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nStarting forward pass...\")\n",
    "    # Forward pass\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = generator(image, watermark)\n",
    "\n",
    "    print(f\"\\nFinal shapes:\")\n",
    "    print(f\"Image input shape: {image.shape}\")\n",
    "    print(f\"Watermark input shape: {watermark.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "    return generator\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_generator()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UaTTPFT4DTtv",
    "outputId": "46effd0b-e598-49db-e563-144ee9e7e0c9",
    "ExecuteTime": {
     "end_time": "2025-02-27T09:07:54.220441Z",
     "start_time": "2025-02-27T09:07:54.049441Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # LeakyReLU\n",
    "        self.leaky_slope = 0.2\n",
    "\n",
    "        # First conv layer: 512x512x3 → 256x256x64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(self.leaky_slope, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Second layer: 256x256x64 → 128x128x128\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(self.leaky_slope, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Third layer: 128x128x128 → 64x64x256\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(self.leaky_slope, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Final layer: 64x64x256 → 31x31x1\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid()  # For binary classification (real/fake)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        x = self.conv1(x)  # → 256x256x64\n",
    "        x = self.conv2(x)  # → 128x128x128\n",
    "        x = self.conv3(x)  # → 64x64x256\n",
    "        x = self.conv4(x)  # → 31x31x1\n",
    "        return x\n",
    "\n",
    "def test_discriminator():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    discriminator = Discriminator().to(device)\n",
    "    torchsummary.summary(discriminator, input_size=(3, 512, 512))\n",
    "\n",
    "    # Save to file\n",
    "    with open(\"discriminator_model_summary.txt\", \"w\") as f:\n",
    "        import sys\n",
    "        sys.stdout = f\n",
    "        torchsummary.summary(discriminator, input_size=(3, 512, 512))\n",
    "        sys.stdout = sys.__stdout__\n",
    "\n",
    "    batch_size = 3\n",
    "    test_input = torch.randn(batch_size, 3, 512, 512).to(device)\n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = discriminator(test_input)\n",
    "\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output value range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "\n",
    "    assert output.min() >= 0 and output.max() <= 1, \"Output values should be in [0,1] range\"\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_discriminator()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4jmqfVhNCx9d",
    "ExecuteTime": {
     "end_time": "2025-02-27T09:07:54.226972Z",
     "start_time": "2025-02-27T09:07:54.225408Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "9UpAMAjgG8Rb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "09324c11-d430-41a3-ba72-b54b4bcb3739",
    "ExecuteTime": {
     "end_time": "2025-02-27T14:30:30.159481Z",
     "start_time": "2025-02-27T09:07:54.232198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple, Dict\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class WatermarkTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator: nn.Module,\n",
    "        discriminator: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        test_loader: DataLoader,\n",
    "        device: torch.device,\n",
    "        learning_rate: float = 0.0002,\n",
    "        beta1: float = 0.5,\n",
    "        beta2: float = 0.999\n",
    "    ):\n",
    "\n",
    "        self.generator = generator.to(device)\n",
    "        self.discriminator = discriminator.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize optimizers\n",
    "        self.g_optimizer = optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=(beta1, beta2)\n",
    "        )\n",
    "        self.d_optimizer = optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=(beta1, beta2)\n",
    "        )\n",
    "\n",
    "        # Loss functions\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "        # tracking variables\n",
    "        self.current_epoch = 0\n",
    "        self.train_history = {\n",
    "            'g_loss': [],\n",
    "            'd_loss': [],\n",
    "            'mse_loss': [],\n",
    "            'adv_loss': []\n",
    "        }\n",
    "\n",
    "    def train_discriminator(\n",
    "        self,\n",
    "        real_images: torch.Tensor,\n",
    "        watermark: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "\n",
    "        batch_size = real_images.size(0)\n",
    "        real_label = torch.ones(batch_size, 1, 31, 31).to(self.device)\n",
    "        fake_label = torch.zeros(batch_size, 1, 31, 31).to(self.device)\n",
    "\n",
    "        # Train on real images\n",
    "        self.d_optimizer.zero_grad()\n",
    "        real_output = self.discriminator(real_images)\n",
    "        d_real_loss = self.bce_loss(real_output, real_label)\n",
    "\n",
    "        # Train on fake (watermarked) images\n",
    "        with torch.no_grad():\n",
    "            fake_images = self.generator(real_images, watermark)\n",
    "        fake_output = self.discriminator(fake_images.detach())\n",
    "        d_fake_loss = self.bce_loss(fake_output, fake_label)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        self.d_optimizer.step()\n",
    "\n",
    "        return d_loss, {\n",
    "            'd_real_loss': d_real_loss.item(),\n",
    "            'd_fake_loss': d_fake_loss.item(),\n",
    "            'd_total_loss': d_loss.item()\n",
    "        }\n",
    "\n",
    "    def train_generator(\n",
    "        self,\n",
    "        real_images: torch.Tensor,\n",
    "        watermark: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Train the generator on a batch of data.\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = real_images.size(0)\n",
    "        real_label = torch.ones(batch_size, 1, 31, 31).to(self.device)\n",
    "\n",
    "        self.g_optimizer.zero_grad()\n",
    "\n",
    "        fake_images = self.generator(real_images, watermark)\n",
    "\n",
    "        # MSE Loss between real and generated images\n",
    "        mse_loss = self.mse_loss(fake_images, real_images)\n",
    "\n",
    "        # Adversarial loss\n",
    "        fake_output = self.discriminator(fake_images)\n",
    "        adv_loss = self.bce_loss(fake_output, real_label)\n",
    "\n",
    "        # Total generator loss (weighted sum)\n",
    "        g_loss = mse_loss + 0.1 * adv_loss\n",
    "        g_loss.backward()\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        return g_loss, {\n",
    "            'mse_loss': mse_loss.item(),\n",
    "            'adv_loss': adv_loss.item(),\n",
    "            'g_total_loss': g_loss.item()\n",
    "        }\n",
    "\n",
    "    def train_epoch(self) -> Dict[str, float]:\n",
    "        \"train on one epoch (for convenience)\"\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        epoch_losses = {\n",
    "            'g_loss': 0.0,\n",
    "            'd_loss': 0.0,\n",
    "            'mse_loss': 0.0,\n",
    "            'adv_loss': 0.0\n",
    "        }\n",
    "\n",
    "        num_batches = len(self.train_loader)\n",
    "        progress_bar = tqdm(self.train_loader, desc=f'Epoch {self.current_epoch+1}')\n",
    "\n",
    "        for batch_idx, (real_images, watermark) in enumerate(progress_bar):\n",
    "            real_images = real_images.to(self.device)\n",
    "            watermark = watermark.to(self.device)\n",
    "\n",
    "            d_loss, d_losses = self.train_discriminator(real_images, watermark)\n",
    "\n",
    "            g_loss, g_losses = self.train_generator(real_images, watermark)\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'D_loss': f\"{d_loss.item():.4f}\",\n",
    "                'G_loss': f\"{g_loss.item():.4f}\"\n",
    "            })\n",
    "\n",
    "            # Accumulate losses\n",
    "            epoch_losses['d_loss'] += d_loss.item()\n",
    "            epoch_losses['g_loss'] += g_loss.item()\n",
    "            epoch_losses['mse_loss'] += g_losses['mse_loss']\n",
    "            epoch_losses['adv_loss'] += g_losses['adv_loss']\n",
    "\n",
    "        # Calculate averages\n",
    "        for key in epoch_losses:\n",
    "            epoch_losses[key] /= num_batches\n",
    "            self.train_history[key].append(epoch_losses[key])\n",
    "\n",
    "        self.current_epoch += 1\n",
    "        return epoch_losses\n",
    "\n",
    "    def train(self, num_epochs: int, save_path: str = None) -> Dict[str, list]:\n",
    "        \"\"\"\n",
    "        Train the model for multiple epochs.\n",
    "\n",
    "        \"\"\"\n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_losses = self.train_epoch()\n",
    "\n",
    "            # Print epoch summary\n",
    "            print(f\"\\nEpoch {self.current_epoch} Summary:\")\n",
    "            for key, value in epoch_losses.items():\n",
    "                print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "            # Save checkpoint\n",
    "            if save_path and (epoch + 1) % 5 == 0:  # Save every 5 epochs\n",
    "                self.save_checkpoint(save_path, epoch + 1)\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {total_time/60:.2f} minutes\")\n",
    "        return self.train_history\n",
    "\n",
    "    def save_checkpoint(self, save_path: str, epoch: int):\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': self.generator.state_dict(),\n",
    "            'discriminator_state_dict': self.discriminator.state_dict(),\n",
    "            'g_optimizer_state_dict': self.g_optimizer.state_dict(),\n",
    "            'd_optimizer_state_dict': self.d_optimizer.state_dict(),\n",
    "            'train_history': self.train_history\n",
    "        }, f\"{save_path}/checkpoint_epoch_{epoch}.pt\")\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path: str):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
    "        self.d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
    "        self.current_epoch = checkpoint['epoch']\n",
    "        self.train_history = checkpoint['train_history']\n",
    "\n",
    "def create_watermark_dataloaders(preprocessor: MRIDatasetPreprocessor, batch_size: int = 32) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Create dataloaders for watermarking training.\"\"\"\n",
    "    train_dataset = WatermarkMRIDataset(preprocessor.train_df)\n",
    "    test_dataset = WatermarkMRIDataset(preprocessor.test_df)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def setup_training(dataset_path: str, batch_size: int = 3):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Initialize preprocessor and create datasets\n",
    "    preprocessor = MRIDatasetPreprocessor(dataset_path)\n",
    "    train_df, test_df = preprocessor.create_dataset_dataframes()\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader, test_loader = create_watermark_dataloaders(preprocessor, batch_size)\n",
    "    # train_loader, test_loader = create_sampled_dataloaders(preprocessor, train_samples=50, test_samples=20, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    # Create models\n",
    "    maxvit = MaxViT(\n",
    "        in_channels=768,\n",
    "        embed_dim=768,\n",
    "        depth=16\n",
    "\n",
    "    ).to(device)\n",
    "\n",
    "    generator = UNetGenerator(maxvit).to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = WatermarkTrainer(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=device\n",
    "    )\n",
    "    from torchsummary import summary\n",
    "\n",
    "    return trainer, preprocessor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer, preprocessor = setup_training(path)\n",
    "\n",
    "    # Train\n",
    "    history = trainer.train(\n",
    "        num_epochs=10,\n",
    "        save_path=\"D:\\\\Dropbox\\\\UMA Augusta\\\\PhD\\\\Research Thesis\\\\20250227_watermarked_checkpoints\"\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing images: 100%|██████████| 5712/5712 [00:11<00:00, 491.65it/s]\n",
      "Analyzing images: 100%|██████████| 1311/1311 [00:02<00:00, 491.92it/s]\n",
      "Analyzing for watermarking: 100%|██████████| 5712/5712 [00:12<00:00, 460.58it/s]\n",
      "Analyzing for watermarking: 100%|██████████| 1311/1311 [00:02<00:00, 473.83it/s]\n",
      "Epoch 1: 100%|██████████| 1904/1904 [18:27<00:00,  1.72it/s, D_loss=0.1697, G_loss=0.2228]\n",
      "Epoch 2: 100%|██████████| 1904/1904 [26:48<00:00,  1.18it/s, D_loss=0.0015, G_loss=0.6904] \n",
      "Epoch 3: 100%|██████████| 1904/1904 [35:20<00:00,  1.11s/it, D_loss=0.0007, G_loss=0.7645]\n",
      "Epoch 4: 100%|██████████| 1904/1904 [31:08<00:00,  1.02it/s, D_loss=0.0001, G_loss=0.9581]\n",
      "Epoch 5: 100%|██████████| 1904/1904 [34:35<00:00,  1.09s/it, D_loss=0.0007, G_loss=0.8292]\n",
      "Epoch 6: 100%|██████████| 1904/1904 [34:40<00:00,  1.09s/it, D_loss=0.0002, G_loss=0.9338]\n",
      "Epoch 7: 100%|██████████| 1904/1904 [37:01<00:00,  1.17s/it, D_loss=0.0001, G_loss=1.0569]\n",
      "Epoch 8: 100%|██████████| 1904/1904 [33:28<00:00,  1.05s/it, D_loss=0.0037, G_loss=0.6259] \n",
      "Epoch 9: 100%|██████████| 1904/1904 [34:04<00:00,  1.07s/it, D_loss=0.0003, G_loss=0.9392]\n",
      "Epoch 10: 100%|██████████| 1904/1904 [34:12<00:00,  1.08s/it, D_loss=0.0002, G_loss=0.8962]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Pb5vLZM5W5KS",
    "ExecuteTime": {
     "end_time": "2025-02-27T14:30:30.417325Z",
     "start_time": "2025-02-27T14:30:30.414945Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "2OwP3dWRW4xC",
    "ExecuteTime": {
     "end_time": "2025-02-27T14:30:30.491851Z",
     "start_time": "2025-02-27T14:30:30.490054Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qn6XmwJ3T0VT",
    "ExecuteTime": {
     "end_time": "2025-02-27T14:30:30.868580Z",
     "start_time": "2025-02-27T14:30:30.527769Z"
    }
   },
   "source": [
    "# PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QvGCbbRdIOVH",
    "ExecuteTime": {
     "end_time": "2025-02-27T14:30:30.876054Z",
     "start_time": "2025-02-27T14:30:30.874226Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
